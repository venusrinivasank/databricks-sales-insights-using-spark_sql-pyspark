from pyspark.sql import SparkSession

df = spark.read.csv('/FileStore/tables/superstore.csv', header = True, inferSchema = True)
display(df)

df.createOrReplaceTempView('sales')

display(spark.sql("""select * from sales limit 10"""))

#widgest creation

dbutils.widgets.dropdown("time_widget",'Monthly',['Monthly','Weekly'])

#collecting start date and end date from the widget created
#importing necessary libraries

from datetime import date,timedelta,datetime
from pyspark.sql.functions import *

time_widget = dbutils.widgets.get("time_widget")
print(time_widget)
today = date.today()
today = today.replace(year=2024)


if time_widget=='Monthly':
    first = today.replace(day=1)
    end_date = first - timedelta(days=1)
    start_date = first - timedelta(days=end_date.day)

else:
    start_date = today - timedelta(days=today.weekday(),weeks=1) - timedelta(days=1)
    end_date = start_date + timedelta(days=6)

print(start_date, end_date)

#how many total number of customer we have

display(spark.sql("""select count(distinct customer_id) as total_no_customer from sales"""))

display(spark.sql(f"""select count(distinct customer_id) as customer_count from sales where order_date between '{start_date}' and '{end_date}'"""))

#total number of order we have

display(spark.sql("""select count(distinct order_id) as count_of_orders from sales"""))

display(spark.sql(f"""select count(distinct order_id) as order_count from sales where order_date between '{start_date}' and '{end_date}'"""))

#total amount of sales and profit

display(\
    spark.sql("""select round(sum(sales),2) as total_sales,round(sum(profit),2) as total_profit from sales""")\
)

display(spark.sql(f"""select round(sum(sales),2) as total_sales, round(sum(profit),2) as total_profit from sales where order_date between '{start_date}' and '{end_date}'"""))

#top sales by country

display(spark.sql("""select round(sum(sales),2) as top_sales, country from sales group by 2 order by 1"""))

display(spark.sql(f"""\
    select round(sum(sales),2) as sales, country from sales 
    where order_date between '{start_date}' and '{end_date}'
    group by 2
    order by 1
"""))

#most profitable region and country

display(spark.sql(f"""\
                    select round(sum(sales),2) as total_sales, region, country from sales
                    group by 2,3
                    order by 1 desc
                    """))

#top sales category products
display(spark.sql("""\
    select round(sum(sales),2) as top_sales, category from sales
    group by 2
    order by 1 desc
    """))

display(df.groupBy('Sub_Category').agg(round(sum('sales'),2).alias('top_sales')).orderBy(col('top_sales').desc()).limit(10))

#most ordered quantity product

display(\
spark.sql("""\
    select round(sum(quantity),2) as total_quantity, product_name from sales 
    group by 2
    order by 1 desc
    limit 10
    """))

#most ordered quantity product

display(\
    df.groupBy('Product_Name').agg(round(sum('Quantity'),2).alias('order_count')).orderBy(col('order_count').desc()).limit(10)\
        )

#most ordered quantity product

display(\
    df.filter((df.Order_Date>=start_date)&(df.Order_Date<=end_date))\
    .groupBy('Product_Name').agg(round(sum('Quantity'),2).alias('order_count')).orderBy(col('order_count').desc()).limit(10)\
        )

#top customer based on sales, city

display(\
    spark.sql(f"""\
        select customer_name, city, round(sum(sales),2) as total_sales from sales 
        group by 1,2
        order by 3 desc
        """)\
)

#top customer based on sales, city

display(\
    spark.sql(f"""\
        select customer_name, city, round(sum(sales),2) as total_sales from sales 
        where order_date between '{start_date}' and '{end_date}'
        group by 1,2
        order by 3 desc
        """)\
)

display(df.groupBy('Customer_Name', 'City').agg(round(sum('Sales'),2).alias('top_sales')).orderBy(col('top_sales').desc()))

